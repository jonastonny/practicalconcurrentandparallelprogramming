4.1.1
	Our results are very much like the ones in 'Microbenchmarks'.
	Most results seems plausible based on the 'Microbencharks'-specifications.
	No significant surprises where found, though we did reach the 16777216 count
	in Mark6.
4.1.2
	Our results are very much like the ones in 'Microbenchmarks'.
	In the case of the 'log' calculation, we reached a higher count on both machines, compared to the one in 'Microbenchmarks'. Other than that, the results were as expected.
4.2.1
	We conductet the tests on two machines, a Mac and a Windows (see specs in seperate files). On the Windows machine we had some strangenesses in the time consumed during the 512-count in the hashCode benchmark, where it used significantly more time than on the Mac. 
	In count 262144 in the uncontended lock, the Mac again had nice consistent benchmarks where the Windows machine spend additional time.
4.2.2
	As seen in the results for this excersise we reach almost exactly the same results as in the 'Microbrenchmarks' article, even though we reach a higher count on the 'log' test.
4.3.1
	OK
4.3.2
	OK. See file output-432.eps
4.3.3
	We find it interesting that the test machine with 8 "cores" performs best with 16 threads (in this particular case). From 16 threads and up it (almost) only takes more time, even though it's not a significant amount. Seemingly 2 threads per logical core seems optimal in this particular setting. 
4.3.4
	The performance of AtomicLong compared to LongCounter, is in our case almost exactly the same (see output-434-combined.eps). The AtomicLong version is a tiny bit faster in almost every case. I would say, that if theres no performance issues, then one should deffinately use the built-in classes and methods, since these are well tested, and we might as well leverage the power of the framework. If we create our own stuff it's prone for error.
4.3.5
	The performance of the local Long compared to the AtomicLong is almost exactly the same (see output-435.eps). 
4.4.1
	Memoizer1	1500094413.6 ns 	 5492380.87		2
4.4.2
	Memoizer2	 866315531.3 ns 	39999378.22		2
4.4.3
	Memoizer3	 725539074.3 ns 	10312332.41		2
4.4.4
	Memoizer4	 708391970.1 ns 	 6798956.49		2
4.4.5
	Memoizer5	 710084590.1 ns 	 7859621.87		2
4.4.6
	Memoizer0	 724245847.0 ns 	 6774683.85		2
4.4.7
	The fastest performing cache implementation in our case is Memoizer4 (see above 4.4.4). This corresponds to the fact, that Memoizer4 is the final implementation of the Memoizer class from the book, using FutureTasks and the atomic PutIfAbsent method (Goetz, 2006, p. 106-108). 
4.4.8
	To meassure scalability we could run the each cache implementation on increasing ranges. This way we would be able to compare the execution times, and see weather size matters for and if some implementation scale better than others. 
